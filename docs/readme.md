# Sales Incentive Analytics Pipeline

## ğŸ“Œ Overview

**Sales Incentive Analytics** is an end-to-end **Data Engineering project** that automates the processing of raw sales data from

**AWS S3 â†’ Spark ETL â†’ MySQL Data Marts â†’ S3 Reporting Zones.**

It simulates a real-world data pipeline for **retail sales and incentive calculation**:

- Ingest raw sales data from **AWS S3**
- Perform **data quality checks** (schema validation, error handling, staging table updates)
- Transform sales data using **PySpark (ETL)**
- Enrich with **dimension tables** (customer, store, product, sales_team)
- Generate two data marts:
  - ğŸ§‘â€ğŸ¤â€ğŸ§‘ **Customer Data Mart** â†’ monthly purchase behavior
  - ğŸ‘¨â€ğŸ’¼ **Sales Team Data Mart** â†’ sales performance + incentives
- Write processed data to **S3 (Parquet)** and **MySQL tables**
- Move files to **processed/error folders** and clean up local staging

---

## ğŸ¯ Project Highlights

This project demonstrates:

âœ… Data Ingestion  
âœ… Data Transformation  
âœ… Data Quality  
âœ… Data Marts  
âœ… Cloud Storage  
âœ… Incentive Analytics

---

## ğŸ› ï¸ Tech Stack

- **Programming:** Python 3, PySpark
- **Data Storage:** AWS S3 (raw, processed, error zones), MySQL (data marts)
- **ETL:** PySpark DataFrames, SQL transformations
- **Workflow:** Boto3 (S3), Spark Session, Custom Utility Modules
- **Logging:** Python logging
- **Deployment:** Local / Cloud-ready

---

```


## Project Architecture
![Pipeline Architecture](docs/Pipeline_Architecture.png)

## Database ER Diagram
![Database ER Diagram](docs/database_schema.drawio.png)



```

```plaintext
Project structure:-
SALES_INCENTIVE_ANALYTICS/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ readme.md
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚    â”œâ”€â”€ config.py
â”‚   â”‚    â””â”€â”€ requirement.txt
â”‚   â””â”€â”€ qa/
â”‚   â”‚    â”œâ”€â”€ config.py
â”‚   â”‚    â””â”€â”€ requirement.txt
â”‚   â””â”€â”€ prod/
â”‚   â”‚    â”œâ”€â”€ config.py
â”‚   â”‚    â””â”€â”€ requirement.txt
â”‚   â”œâ”€â”€ sql_scripts/
â”‚   â”‚    â””â”€â”€ table_scripts.sql
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚    â”œâ”€â”€ __init__.py
â”‚   â”‚    â””â”€â”€ delete/
â”‚   â”‚    â”‚      â”œâ”€â”€ aws_delete.py
â”‚   â”‚    â”‚      â”œâ”€â”€ database_delete.py
â”‚   â”‚    â”‚      â””â”€â”€ local_file_delete.py
â”‚   â”‚    â””â”€â”€ download/
â”‚   â”‚    â”‚      â””â”€â”€ aws_file_download.py
â”‚   â”‚    â””â”€â”€ move/
â”‚   â”‚    â”‚      â””â”€â”€ move_files.py
â”‚   â”‚    â””â”€â”€ read/
â”‚   â”‚    â”‚      â”œâ”€â”€ aws_read.py
â”‚   â”‚    â”‚      â””â”€â”€ database_read.py
â”‚   â”‚    â””â”€â”€ transformations/
â”‚   â”‚    â”‚      â””â”€â”€ jobs/
â”‚   â”‚    â”‚      â”‚     â”œâ”€â”€ customer_mart_sql_transform_write.py
â”‚   â”‚    â”‚      â”‚     â”œâ”€â”€ dimension_tables_join.py
â”‚   â”‚    â”‚      â”‚     â”œâ”€â”€ main.py
â”‚   â”‚    â”‚      â”‚     â””â”€â”€sales_mart_sql_transform_write.py
â”‚   â”‚    â””â”€â”€ upload/
â”‚   â”‚    â”‚      â””â”€â”€ upload_to_s3.py
â”‚   â”‚    â””â”€â”€ utility/
â”‚   â”‚    â”‚      â”œâ”€â”€ encrypt_decrypt.py
â”‚   â”‚    â”‚      â”œâ”€â”€ logging_config.py
â”‚   â”‚    â”‚      â”œâ”€â”€ s3_client_object.py
â”‚   â”‚    â”‚      â”œâ”€â”€ spark_session.py
â”‚   â”‚    â”‚      â””â”€â”€ my_sql_session.py
â”‚   â”‚    â””â”€â”€ write/
â”‚   â”‚    â”‚      â”œâ”€â”€ database_write.py
â”‚   â”‚    â”‚      â””â”€â”€ parquet_write.py
â”‚   â”œâ”€â”€ test/
â”‚   â”‚    â”œâ”€â”€ scratch_pad.py.py
â”‚   â”‚    â””â”€â”€ generate_csv_data.py
```

## âš™ï¸ How the Pipeline Works

### ğŸ”¹ Step 0: Data Generation (Simulation)

Since real retail sales data is not readily available, this project **generates synthetic sales data** using custom Python scripts.

- Generates randomized Customer, Store, Product, and Sales Team data
- Creates transactional sales CSV files (with realistic schema)
- Uploads generated files to **AWS S3 (raw zone)**  
  ğŸ‘‰ This simulates a real-world system continuously pushing sales data into S3.

---

### ğŸ”¹ Step 1: S3 â†’ Local (Ingestion)

- Connects to AWS S3 (via Boto3)
- Lists and downloads sales CSVs
- Validates mandatory schema columns

### ğŸ”¹ Step 2: Data Quality & Staging

- Moves bad files to `error/` folder (locally + S3)
- Updates MySQL staging table with file status (Active/Inactive)

### ğŸ”¹ Step 3: Transformation (PySpark)

- Cleans extra columns â†’ consolidates into `additional_column`
- Joins with dimension tables (Customer, Store, Product, Sales Team)
- Produces **Fact Table** of enriched sales transactions

### ğŸ”¹ Step 4: Data Marts

- **Customer Data Mart** â†’ monthly purchase summary per customer
- **Sales Team Data Mart** â†’ monthly sales per person + incentive calculation (top performers get 1% of sales)

### ğŸ”¹ Step 5: Write to Storage

- Local Parquet â†’ Upload to S3 (processed zone)
- Partitioned data by `sales_month` & `store_id` for efficient querying
- Insert results into MySQL Data Mart tables

### ğŸ”¹ Step 6: Cleanup

- Moves raw files â†’ `processed/` S3 folder
- Deletes temporary local files

---

````
## ğŸ§‘â€ğŸ’» Setup & Run

### 1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/your-username/sales-incentive-analytics.git
cd sales-incentive-analytics
````

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure `resources/dev/config.py`

Update values for:

- AWS credentials (encrypted)
- MySQL connection (url, user, password)
- Bucket names, folder paths
- Mandatory schema columns

### 4ï¸âƒ£ Run pipeline

```bash
python src/main/main.py
```

---

## âœ… Features

- Automated ETL pipeline (**raw â†’ processed â†’ marts**)
- Data quality checks (missing schema, bad files segregation)
- Encrypted AWS credentials
- Parquet + partitioned storage for scalability
- MySQL integration for analytics & reporting

---

## ğŸš€ Future Enhancements

- Orchestrate with **Apache Airflow**
- Store marts in **Snowflake / Redshift**
- Add **Power BI / Tableau dashboards**
- Implement **CI/CD (GitHub Actions)**
- Add **unit tests (pytest)**

---

## ğŸ“„ License

MIT License

---

## ğŸ™Œ Author

ğŸ‘¤Yashwant Yadav
ğŸ“§ \[[yashwantyadav2003@gmail.com]]
ğŸ”— [LinkedIn](https://www.linkedin.com/in/yashwant-yadav14) | [GitHub](https://github.com/Yashwant-14)
